{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.contrib import layers\n",
    "from tensorflow.contrib import losses\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from sklearn import metrics\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle # randomizing ordered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMG_WIDTH, IMG_HEIGHT = 100, 100 # dimensions of our images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set logging level to info to see detailed log output\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pepperoni_data = np.load('data/pepperoni_rgb_data.npy')\n",
    "sausage_data = np.load('data/sausage_rgb_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ array([[[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       ..., \n",
       "       [[ 29,  43, 138],\n",
       "        [ 62,  73, 168],\n",
       "        [ 83,  92, 173],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[ 52,  68, 164],\n",
       "        [ 56,  74, 165],\n",
       "        [ 72,  80, 166],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[  8,  29, 120],\n",
       "        [ 52,  75, 161],\n",
       "        [ 55,  66, 135],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]]], dtype=uint8),\n",
       "        array([1, 0])],\n",
       "       [ array([[[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ..., \n",
       "        [ 94, 147, 210],\n",
       "        [ 99, 138, 217],\n",
       "        [107, 151, 204]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ..., \n",
       "        [ 94, 147, 210],\n",
       "        [ 98, 137, 216],\n",
       "        [106, 150, 204]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ..., \n",
       "        [ 92, 145, 208],\n",
       "        [ 98, 137, 215],\n",
       "        [104, 148, 200]],\n",
       "\n",
       "       ..., \n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ..., \n",
       "        [  7,  13,  50],\n",
       "        [ 11,  35, 112],\n",
       "        [  6,   4,  73]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ..., \n",
       "        [  2,   7,  59],\n",
       "        [  0,  30,  98],\n",
       "        [  0,   3, 101]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ..., \n",
       "        [ 19,  26,  80],\n",
       "        [  0,  20, 100],\n",
       "        [  0,  25, 102]]], dtype=uint8),\n",
       "        array([1, 0])],\n",
       "       [ array([[[ 29,  70, 127],\n",
       "        [ 82, 119, 187],\n",
       "        [ 63, 112, 174],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[ 17,  51, 117],\n",
       "        [ 21,  55, 121],\n",
       "        [ 13,  47, 111],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[ 21,  50, 126],\n",
       "        [ 22,  53, 123],\n",
       "        [ 19,  48, 115],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       ..., \n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]]], dtype=uint8),\n",
       "        array([1, 0])],\n",
       "       ..., \n",
       "       [ array([[[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       ..., \n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]]], dtype=uint8),\n",
       "        array([1, 0])],\n",
       "       [ array([[[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       ..., \n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]]], dtype=uint8),\n",
       "        array([1, 0])],\n",
       "       [ array([[[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       ..., \n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ..., \n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ..., \n",
       "        [0, 1, 2],\n",
       "        [0, 1, 2],\n",
       "        [0, 1, 2]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 1],\n",
       "        [0, 0, 1],\n",
       "        [0, 1, 2]]], dtype=uint8),\n",
       "        array([1, 0])]], dtype=object)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pepperoni_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ array([[[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       ..., \n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ..., \n",
       "        [ 14,  79, 130],\n",
       "        [ 25,  98, 165],\n",
       "        [ 15,  48, 157]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ..., \n",
       "        [ 14,  53, 165],\n",
       "        [  8,  61, 127],\n",
       "        [ 12,  38, 139]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ..., \n",
       "        [ 28,  68, 142],\n",
       "        [ 20,  20, 150],\n",
       "        [  6,  17,  69]]], dtype=uint8),\n",
       "        array([0, 1])],\n",
       "       [ array([[[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       ..., \n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ..., \n",
       "        [1, 1, 1],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ..., \n",
       "        [1, 1, 1],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ..., \n",
       "        [2, 2, 2],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]]], dtype=uint8),\n",
       "        array([0, 1])],\n",
       "       [ array([[[169, 218, 220],\n",
       "        [169, 217, 223],\n",
       "        [155, 200, 212],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[167, 209, 221],\n",
       "        [149, 198, 225],\n",
       "        [130, 171, 176],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[ 45,  82, 113],\n",
       "        [ 76, 116, 157],\n",
       "        [171, 206, 209],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       ..., \n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]]], dtype=uint8),\n",
       "        array([0, 1])],\n",
       "       ..., \n",
       "       [ array([[[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       ..., \n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ..., \n",
       "        [ 96, 170, 220],\n",
       "        [ 82, 137, 203],\n",
       "        [ 45,  66, 112]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ..., \n",
       "        [116, 165, 218],\n",
       "        [ 85, 135, 192],\n",
       "        [ 92, 138, 196]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ..., \n",
       "        [ 67, 120, 190],\n",
       "        [ 94, 142, 195],\n",
       "        [123, 159, 204]]], dtype=uint8),\n",
       "        array([0, 1])],\n",
       "       [ array([[[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       ..., \n",
       "       [[  4,   3,   9],\n",
       "        [149, 157, 169],\n",
       "        [ 42,  51,  97],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  1,   1,   1],\n",
       "        [104, 105, 110],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  2,   1,   6],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]]], dtype=uint8),\n",
       "        array([0, 1])],\n",
       "       [ array([[[112, 140, 178],\n",
       "        [ 59,  99, 137],\n",
       "        [ 34,  83, 126],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[121, 159, 183],\n",
       "        [135, 181, 215],\n",
       "        [129, 177, 199],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[ 86, 123, 158],\n",
       "        [128, 173, 202],\n",
       "        [138, 186, 210],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       ..., \n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ..., \n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]]], dtype=uint8),\n",
       "        array([0, 1])]], dtype=object)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sausage_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_data = np.concatenate((pepperoni_data, sausage_data), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2002, 2)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shuffle(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = full_data[0:1500]\n",
    "validation = full_data[1501:1750]\n",
    "test = full_data[-250:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For Gray Scale Images\n",
    "X_train = np.array([i[0] for i in train], dtype='f').reshape(-1,IMG_WIDTH,IMG_HEIGHT,1)\n",
    "y_train = np.array([i[1] for i in train], dtype='int32')\n",
    "\n",
    "X_validation = np.array([i[0] for i in validation], dtype='f').reshape(-1,IMG_WIDTH,IMG_HEIGHT,1)\n",
    "y_validation = np.array([i[1] for i in validation], dtype='int32')\n",
    "\n",
    "X_test = np.array([i[0] for i in test], dtype='f').reshape(-1, IMG_WIDTH, IMG_HEIGHT, 1)\n",
    "y_test = np.array([i[1] for i in test], dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For RGB, 3 channel layout\n",
    "X_train = np.array([i[0] for i in train], dtype='f').reshape(-1,IMG_WIDTH,IMG_HEIGHT,3)\n",
    "y_train = np.array([i[1] for i in train], dtype='int32')\n",
    "\n",
    "X_validation = np.array([i[0] for i in validation], dtype='f').reshape(-1,IMG_WIDTH,IMG_HEIGHT,3)\n",
    "y_validation = np.array([i[1] for i in validation], dtype='int32')\n",
    "\n",
    "X_test = np.array([i[0] for i in test], dtype='f').reshape(-1, IMG_WIDTH, IMG_HEIGHT, 3)\n",
    "y_test = np.array([i[1] for i in test], dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 100, 100, 3)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# creating custom estimator\n",
    "def model_function_2d(features, targets, mode):\n",
    "    #input layer \n",
    "    #Reshape features to 4-D tensor: [batch_size, width, height, channels]\n",
    "    # MNIST images are 28x28 pixels, and have one color channel \n",
    "    #batch_size corresponds to number of images: -1 represents compute the number of images automatically\n",
    "    input_layer = tf.reshape(features, [-1, IMG_WIDTH, IMG_HEIGHT, 1])\n",
    "    kernel_size_1=[5, 5, 5]\n",
    "    \n",
    "    # Convolutional Layer #1\n",
    "    # Computes 32 features using a 5x5 filter with ReLU activation.\n",
    "    # Padding is added to preserve width and height.\n",
    "    # Input Tensor Shape: [batch_size, 28, 28, 1]\n",
    "    # Output Tensor Shape: [batch_size, 28, 28, 32]\n",
    "    conv1 = tf.layers.conv3d(\n",
    "      inputs=input_layer,\n",
    "      num_of_outputs=32,\n",
    "      kernel_size=kernel_size_1,\n",
    "      stride=1,\n",
    "      padding=\"SAME\",\n",
    "      activation_fn=tf.nn.relu)\n",
    "    \n",
    "    # Pooling Layer #1\n",
    "    # First max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 28, 28, 32]\n",
    "    # Output Tensor Shape: [batch_size, 14, 14, 32]\n",
    "    pool1 = layers.max_pool2d(inputs=conv1, kernel_size=[2, 2], stride=2)\n",
    "    \n",
    "    # Convolutional Layer #2\n",
    "    # Computes 64 features using a 5x5 filter.\n",
    "    # Padding is added to preserve width and height.\n",
    "    # Input Tensor Shape: [batch_size, 14, 14, 32]\n",
    "    # Output Tensor Shape: [batch_size, 14, 14, 64]\n",
    "    conv2 = layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      num_outputs=64,\n",
    "      kernel_size=[5, 5],\n",
    "      stride=1,\n",
    "      padding=\"SAME\",\n",
    "      activation_fn=tf.nn.relu)\n",
    "    # Pooling Layer #2\n",
    "    # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 14, 14, 64]\n",
    "    # Output Tensor Shape: [batch_size, 7, 7, 64]\n",
    "    pool2 = layers.max_pool2d(inputs=conv2, kernel_size=[2, 2], stride=2)\n",
    "    \n",
    "    # Flatten tensor into a batch of vectors\n",
    "    # Input Tensor Shape: [batch_size, 7, 7, 64]\n",
    "    # Output Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "    \n",
    "    pool2_flat = tf.reshape(pool2, [-1, int(IMG_WIDTH/4) * int(IMG_HEIGHT/4) * 64])\n",
    "    \n",
    "    # Fully connected Layers with 100, 20 neurons\n",
    "    # Input Tensor Shapuntitled0.e: [batch_size, 14 * 14 * 32]\n",
    "    # Output Tensor Shape: [batch_size, 10]\n",
    "    fclayers = layers.stack(pool2_flat, layers.fully_connected, [100,20], activation_fn = tf.nn.relu)\n",
    "    outputs = layers.fully_connected(inputs=fclayers,\n",
    "                                                 num_outputs=2,\n",
    "                                                 activation_fn=None)\n",
    "    # Calculate loss using mean squared error\n",
    "    loss = losses.softmax_cross_entropy(outputs, targets)\n",
    "    # Create an optimizer for minimizing the loss function\n",
    "    optimizer = layers.optimize_loss(\n",
    "      loss=loss,\n",
    "      global_step=tf.contrib.framework.get_global_step(),\n",
    "      learning_rate=0.8,\n",
    "      optimizer=\"SGD\")\n",
    "    probs = tf.nn.softmax(outputs)\n",
    "    \n",
    "    return {'probs':probs, 'labels':tf.arg_max(probs,1)}, loss, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For VIDEOS\n",
    "# creating custom estimator\n",
    "def model_function_3d_for_videos(features, targets, mode):\n",
    "    #input layer \n",
    "    #Reshape features to 4-D tensor: [batch_size, width, height, channels]\n",
    "    # MNIST images are 28x28 pixels, and have one color channel \n",
    "    #batch_size corresponds to number of images: -1 represents compute the number of images automatically\n",
    "    input_layer = tf.reshape(features, [-1, IMG_WIDTH, IMG_HEIGHT, 3])\n",
    "    kernel_size_1 = [5, 5, 5]\n",
    "    kernel_size_2 = [5, 5, 5]\n",
    "    \n",
    "    # Convolutional Layer #1\n",
    "    # Computes 32 features using a 5x5 filter with ReLU activation.\n",
    "    # Padding is added to preserve width and height.\n",
    "    # Input Tensor Shape: [batch_size, 28, 28, 1]\n",
    "    # Output Tensor Shape: [batch_size, 28, 28, 32]\n",
    "    conv1 = tf.layers.conv3d(\n",
    "      inputs=input_layer,\n",
    "      filters=32,\n",
    "      kernel_size=kernel_size_1,\n",
    "      strides=(1,1,1),\n",
    "      padding=\"SAME\",\n",
    "      activation=tf.nn.relu)\n",
    "    \n",
    "    # Pooling Layer #1\n",
    "    # First max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 28, 28, 32]\n",
    "    # Output Tensor Shape: [batch_size, 14, 14, 32]\n",
    "    pool1 = tf.layers.max_pooling3d(inputs=conv1, pool_size=[2, 2, 2], strides=(2,2,2))\n",
    "    \n",
    "    # Convolutional Layer #2\n",
    "    # Computes 64 features using a 5x5 filter.\n",
    "    # Padding is added to preserve width and height.\n",
    "    # Input Tensor Shape: [batch_size, 14, 14, 32]\n",
    "    # Output Tensor Shape: [batch_size, 14, 14, 64]\n",
    "    conv2 = tf.layers.conv3d(\n",
    "      inputs=pool1,\n",
    "      num_outputs=64,\n",
    "      kernel_size=kernel_size_2,\n",
    "      strides=(1,1,1),\n",
    "      padding=\"SAME\",\n",
    "      activation_fn=tf.nn.relu)\n",
    "    # Pooling Layer #2\n",
    "    # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 14, 14, 64]\n",
    "    # Output Tensor Shape: [batch_size, 7, 7, 64]\n",
    "    pool2 = tf.layers.max_pooling3d(inputs=conv2, pool_size=[2, 2, 2], strides=(2,2,2))\n",
    "    \n",
    "    # Flatten tensor into a batch of vectors\n",
    "    # Input Tensor Shape: [batch_size, 7, 7, 64]\n",
    "    # Output Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "    \n",
    "    pool2_flat = tf.reshape(pool2, [-1, int(IMG_WIDTH/4) * int(IMG_HEIGHT/4) * 64 * 3])\n",
    "    \n",
    "    # Fully connected Layers with 100, 20 neurons\n",
    "    # Input Tensor Shapuntitled0.e: [batch_size, 14 * 14 * 32]\n",
    "    # Output Tensor Shape: [batch_size, 10]\n",
    "    fclayers = layers.stack(pool2_flat, layers.fully_connected, [100,20], activation_fn = tf.nn.relu)\n",
    "    outputs = layers.fully_connected(inputs=fclayers,\n",
    "                                                 num_outputs=2,\n",
    "                                                 activation_fn=None)\n",
    "    # Calculate loss using mean squared error\n",
    "    loss = losses.softmax_cross_entropy(outputs, targets)\n",
    "    # Create an optimizer for minimizing the loss function\n",
    "    optimizer = layers.optimize_loss(\n",
    "      loss=loss,\n",
    "      global_step=tf.contrib.framework.get_global_step(),\n",
    "      learning_rate=0.8,\n",
    "      optimizer=\"SGD\")\n",
    "    probs = tf.nn.softmax(outputs)\n",
    "    \n",
    "    return {'probs':probs, 'labels':tf.arg_max(probs,1)}, loss, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For Images\n",
    "# creating custom estimator\n",
    "def model_function(features, targets, mode):\n",
    "    #input layer \n",
    "    #Reshape features to 4-D tensor: [batch_size, width, height, channels]\n",
    "    # MNIST images are 28x28 pixels, and have one color channel \n",
    "    #batch_size corresponds to number of images: -1 represents compute the number of images automatically\n",
    "    input_layer = tf.reshape(features, [-1, IMG_WIDTH, IMG_HEIGHT, 1])\n",
    "    kernel_size_1=[3, 3]\n",
    "    \n",
    "    # Convolutional Layer #1\n",
    "    # Computes 32 features using a 5x5 filter with ReLU activation.\n",
    "    # Padding is added to preserve width and height.\n",
    "    # Input Tensor Shape: [batch_size, 28, 28, 1]\n",
    "    # Output Tensor Shape: [batch_size, 28, 28, 32]\n",
    "    conv1 = layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      num_outputs=32,\n",
    "      kernel_size=kernel_size_1,\n",
    "      stride=1,\n",
    "      padding=\"SAME\",\n",
    "      activation_fn=tf.nn.relu)\n",
    "    \n",
    "    # Pooling Layer #1\n",
    "    # First max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 28, 28, 32]\n",
    "    # Output Tensor Shape: [batch_size, 14, 14, 32]\n",
    "    pool1 = layers.max_pool2d(inputs=conv1, kernel_size=[2, 2], stride=2)\n",
    "    \n",
    "    # Convolutional Layer #2\n",
    "    # Computes 64 features using a 5x5 filter.\n",
    "    # Padding is added to preserve width and height.\n",
    "    # Input Tensor Shape: [batch_size, 14, 14, 32]\n",
    "    # Output Tensor Shape: [batch_size, 14, 14, 64]\n",
    "    conv2 = layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      num_outputs=64,\n",
    "      kernel_size=[2, 2],\n",
    "      stride=1,\n",
    "      padding=\"SAME\",\n",
    "      activation_fn=tf.nn.relu)\n",
    "    # Pooling Layer #2\n",
    "    # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 14, 14, 64]\n",
    "    # Output Tensor Shape: [batch_size, 7, 7, 64]\n",
    "    pool2 = layers.max_pool2d(inputs=conv2, kernel_size=[2, 2], stride=2)\n",
    "    \n",
    "    # Flatten tensor into a batch of vectors\n",
    "    # Input Tensor Shape: [batch_size, 7, 7, 64]\n",
    "    # Output Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "    \n",
    "    pool2_flat = tf.reshape(pool2, [-1, int(IMG_WIDTH/4) * int(IMG_HEIGHT/4) * 64 * 3 ])\n",
    "    \n",
    "    # Fully connected Layers with 100, 20 neurons\n",
    "    # Input Tensor Shapuntitled0.e: [batch_size, 14 * 14 * 32]\n",
    "    # Output Tensor Shape: [batch_size, 10]\n",
    "    fclayers = layers.stack(pool2_flat, layers.fully_connected, [200,40], activation_fn = tf.nn.relu)\n",
    "    outputs = layers.fully_connected(inputs=fclayers,\n",
    "                                                 num_outputs=2,\n",
    "                                                 activation_fn=None)\n",
    "    # Calculate loss using mean squared error\n",
    "    loss = losses.sigmoid_cross_entropy(outputs, targets)\n",
    "    # Create an optimizer for minimizing the loss function\n",
    "    optimizer = layers.optimize_loss(\n",
    "      loss=loss,\n",
    "      global_step=tf.contrib.framework.get_global_step(),\n",
    "      learning_rate=0.1,\n",
    "      optimizer=\"SGD\")\n",
    "    probs = tf.nn.softmax(outputs)\n",
    "    \n",
    "    return {'probs':probs, 'labels':tf.arg_max(probs,1)}, loss, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f2f981f8be0>, '_master': '', '_num_ps_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000}\n"
     ]
    }
   ],
   "source": [
    "#create custom estimator\n",
    "nn = learn.Estimator(model_fn=model_function, model_dir=\"/home/datascience/projects/vsoft/tf_model_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-93-7d47b909c7d9>:2: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-93-7d47b909c7d9>:2: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-93-7d47b909c7d9>:2: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with batch_size is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-90-d55e44b21b3e>:62: sigmoid_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.sigmoid_cross_entropy instead.\n",
      "WARNING:tensorflow:From /usr/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:346: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n",
      "WARNING:tensorflow:From /usr/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:247: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  equality = a == b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 4 into /home/datascience/projects/vsoft/tf_model_new/model.ckpt.\n"
     ]
    }
   ],
   "source": [
    "#build the model\n",
    "nn.fit(x=X_train, y=y_train, steps=100, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for var in nn.get_variable_names():\n",
    "    print (\"%s:%s\" % (var,nn.get_variable_value(var)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-56-4901adafbb12>:2: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-53-bac528550eab>:62: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead.\n",
      "WARNING:tensorflow:From /usr/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:394: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n",
      "WARNING:tensorflow:From /usr/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:247: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  equality = a == b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction 1: [ 0.  1.] : 1\n",
      "Prediction 2: [ 0.  1.] : 1\n",
      "Prediction 3: [ 0.  1.] : 1\n",
      "Prediction 4: [ 0.  1.] : 1\n",
      "Prediction 5: [ 0.  1.] : 1\n",
      "Prediction 6: [ 0.  1.] : 1\n",
      "Prediction 7: [ 0.  1.] : 1\n",
      "Prediction 8: [ 0.  1.] : 1\n",
      "Prediction 9: [ 0.  1.] : 1\n",
      "Prediction 10: [ 0.  1.] : 1\n",
      "Prediction 11: [ 0.  1.] : 1\n",
      "Prediction 12: [ 0.  1.] : 1\n",
      "Prediction 13: [ 0.  1.] : 1\n",
      "Prediction 14: [ 0.  1.] : 1\n",
      "Prediction 15: [ 0.  1.] : 1\n",
      "Prediction 16: [ 0.  1.] : 1\n",
      "Prediction 17: [ 0.  1.] : 1\n",
      "Prediction 18: [ 0.  1.] : 1\n",
      "Prediction 19: [ 0.  1.] : 1\n",
      "Prediction 20: [ 0.  1.] : 1\n",
      "Prediction 21: [ 0.  1.] : 1\n",
      "Prediction 22: [ 0.  1.] : 1\n",
      "Prediction 23: [ 0.  1.] : 1\n",
      "Prediction 24: [ 0.  1.] : 1\n",
      "Prediction 25: [ 0.  1.] : 1\n",
      "Prediction 26: [ 0.  1.] : 1\n",
      "Prediction 27: [ 0.  1.] : 1\n",
      "Prediction 28: [ 0.  1.] : 1\n",
      "Prediction 29: [ 0.  1.] : 1\n",
      "Prediction 30: [ 0.  1.] : 1\n",
      "Prediction 31: [ 0.  1.] : 1\n",
      "Prediction 32: [ 0.  1.] : 1\n",
      "Prediction 33: [ 0.  1.] : 1\n",
      "Prediction 34: [ 0.  1.] : 1\n",
      "Prediction 35: [ 0.  1.] : 1\n",
      "Prediction 36: [ 0.  1.] : 1\n",
      "Prediction 37: [ 0.  1.] : 1\n",
      "Prediction 38: [ 0.  1.] : 1\n",
      "Prediction 39: [ 0.  1.] : 1\n",
      "Prediction 40: [ 0.  1.] : 1\n",
      "Prediction 41: [ 0.  1.] : 1\n",
      "Prediction 42: [ 0.  1.] : 1\n",
      "Prediction 43: [ 0.  1.] : 1\n",
      "Prediction 44: [ 0.  1.] : 1\n",
      "Prediction 45: [ 0.  1.] : 1\n",
      "Prediction 46: [ 0.  1.] : 1\n",
      "Prediction 47: [ 0.  1.] : 1\n",
      "Prediction 48: [ 0.  1.] : 1\n",
      "Prediction 49: [ 0.  1.] : 1\n",
      "Prediction 50: [ 0.  1.] : 1\n",
      "Prediction 51: [ 0.  1.] : 1\n",
      "Prediction 52: [ 0.  1.] : 1\n",
      "Prediction 53: [ 0.  1.] : 1\n",
      "Prediction 54: [ 0.  1.] : 1\n",
      "Prediction 55: [ 0.  1.] : 1\n",
      "Prediction 56: [ 0.  1.] : 1\n",
      "Prediction 57: [ 0.  1.] : 1\n",
      "Prediction 58: [ 0.  1.] : 1\n",
      "Prediction 59: [ 0.  1.] : 1\n",
      "Prediction 60: [ 0.  1.] : 1\n",
      "Prediction 61: [ 0.  1.] : 1\n",
      "Prediction 62: [ 0.  1.] : 1\n",
      "Prediction 63: [ 0.  1.] : 1\n",
      "Prediction 64: [ 0.  1.] : 1\n",
      "Prediction 65: [ 0.  1.] : 1\n",
      "Prediction 66: [ 0.  1.] : 1\n",
      "Prediction 67: [ 0.  1.] : 1\n",
      "Prediction 68: [ 0.  1.] : 1\n",
      "Prediction 69: [ 0.  1.] : 1\n",
      "Prediction 70: [ 0.  1.] : 1\n",
      "Prediction 71: [ 0.  1.] : 1\n",
      "Prediction 72: [ 0.  1.] : 1\n",
      "Prediction 73: [ 0.  1.] : 1\n",
      "Prediction 74: [ 0.  1.] : 1\n",
      "Prediction 75: [ 0.  1.] : 1\n",
      "Prediction 76: [ 0.  1.] : 1\n",
      "Prediction 77: [ 0.  1.] : 1\n",
      "Prediction 78: [ 0.  1.] : 1\n",
      "Prediction 79: [ 0.  1.] : 1\n",
      "Prediction 80: [ 0.  1.] : 1\n",
      "Prediction 81: [ 0.  1.] : 1\n",
      "Prediction 82: [ 0.  1.] : 1\n",
      "Prediction 83: [ 0.  1.] : 1\n",
      "Prediction 84: [ 0.  1.] : 1\n",
      "Prediction 85: [ 0.  1.] : 1\n",
      "Prediction 86: [ 0.  1.] : 1\n",
      "Prediction 87: [ 0.  1.] : 1\n",
      "Prediction 88: [ 0.  1.] : 1\n",
      "Prediction 89: [ 0.  1.] : 1\n",
      "Prediction 90: [ 0.  1.] : 1\n",
      "Prediction 91: [ 0.  1.] : 1\n",
      "Prediction 92: [ 0.  1.] : 1\n",
      "Prediction 93: [ 0.  1.] : 1\n",
      "Prediction 94: [ 0.  1.] : 1\n",
      "Prediction 95: [ 0.  1.] : 1\n",
      "Prediction 96: [ 0.  1.] : 1\n",
      "Prediction 97: [ 0.  1.] : 1\n",
      "Prediction 98: [ 0.  1.] : 1\n",
      "Prediction 99: [ 0.  1.] : 1\n",
      "Prediction 100: [ 0.  1.] : 1\n",
      "Prediction 101: [ 0.  1.] : 1\n",
      "Prediction 102: [ 0.  1.] : 1\n",
      "Prediction 103: [ 0.  1.] : 1\n",
      "Prediction 104: [ 0.  1.] : 1\n",
      "Prediction 105: [ 0.  1.] : 1\n",
      "Prediction 106: [ 0.  1.] : 1\n",
      "Prediction 107: [ 0.  1.] : 1\n",
      "Prediction 108: [ 0.  1.] : 1\n",
      "Prediction 109: [ 0.  1.] : 1\n",
      "Prediction 110: [ 0.  1.] : 1\n",
      "Prediction 111: [ 0.  1.] : 1\n",
      "Prediction 112: [ 0.  1.] : 1\n",
      "Prediction 113: [ 0.  1.] : 1\n",
      "Prediction 114: [ 0.  1.] : 1\n",
      "Prediction 115: [ 0.  1.] : 1\n",
      "Prediction 116: [ 0.  1.] : 1\n",
      "Prediction 117: [ 0.  1.] : 1\n",
      "Prediction 118: [ 0.  1.] : 1\n",
      "Prediction 119: [ 0.  1.] : 1\n",
      "Prediction 120: [ 0.  1.] : 1\n",
      "Prediction 121: [ 0.  1.] : 1\n",
      "Prediction 122: [ 0.  1.] : 1\n",
      "Prediction 123: [ 0.  1.] : 1\n",
      "Prediction 124: [ 0.  1.] : 1\n",
      "Prediction 125: [ 0.  1.] : 1\n",
      "Prediction 126: [ 0.  1.] : 1\n",
      "Prediction 127: [ 0.  1.] : 1\n",
      "Prediction 128: [ 0.  1.] : 1\n",
      "Prediction 129: [ 0.  1.] : 1\n",
      "Prediction 130: [ 0.  1.] : 1\n",
      "Prediction 131: [ 0.  1.] : 1\n",
      "Prediction 132: [ 0.  1.] : 1\n",
      "Prediction 133: [ 0.  1.] : 1\n",
      "Prediction 134: [ 0.  1.] : 1\n",
      "Prediction 135: [ 0.  1.] : 1\n",
      "Prediction 136: [ 0.  1.] : 1\n",
      "Prediction 137: [ 0.  1.] : 1\n",
      "Prediction 138: [ 0.  1.] : 1\n",
      "Prediction 139: [ 0.  1.] : 1\n",
      "Prediction 140: [ 0.  1.] : 1\n",
      "Prediction 141: [ 0.  1.] : 1\n",
      "Prediction 142: [ 0.  1.] : 1\n",
      "Prediction 143: [ 0.  1.] : 1\n",
      "Prediction 144: [ 0.  1.] : 1\n",
      "Prediction 145: [ 0.  1.] : 1\n",
      "Prediction 146: [ 0.  1.] : 1\n",
      "Prediction 147: [ 0.  1.] : 1\n",
      "Prediction 148: [ 0.  1.] : 1\n",
      "Prediction 149: [ 0.  1.] : 1\n",
      "Prediction 150: [ 0.  1.] : 1\n",
      "Prediction 151: [ 0.  1.] : 1\n",
      "Prediction 152: [ 0.  1.] : 1\n",
      "Prediction 153: [ 0.  1.] : 1\n",
      "Prediction 154: [ 0.  1.] : 1\n",
      "Prediction 155: [ 0.  1.] : 1\n",
      "Prediction 156: [ 0.  1.] : 1\n",
      "Prediction 157: [ 0.  1.] : 1\n",
      "Prediction 158: [ 0.  1.] : 1\n",
      "Prediction 159: [ 0.  1.] : 1\n",
      "Prediction 160: [ 0.  1.] : 1\n",
      "Prediction 161: [ 0.  1.] : 1\n",
      "Prediction 162: [ 0.  1.] : 1\n",
      "Prediction 163: [ 0.  1.] : 1\n",
      "Prediction 164: [ 0.  1.] : 1\n",
      "Prediction 165: [ 0.  1.] : 1\n",
      "Prediction 166: [ 0.  1.] : 1\n",
      "Prediction 167: [ 0.  1.] : 1\n",
      "Prediction 168: [ 0.  1.] : 1\n",
      "Prediction 169: [ 0.  1.] : 1\n",
      "Prediction 170: [ 0.  1.] : 1\n",
      "Prediction 171: [ 0.  1.] : 1\n",
      "Prediction 172: [ 0.  1.] : 1\n",
      "Prediction 173: [ 0.  1.] : 1\n",
      "Prediction 174: [ 0.  1.] : 1\n",
      "Prediction 175: [ 0.  1.] : 1\n",
      "Prediction 176: [ 0.  1.] : 1\n",
      "Prediction 177: [ 0.  1.] : 1\n",
      "Prediction 178: [ 0.  1.] : 1\n",
      "Prediction 179: [ 0.  1.] : 1\n",
      "Prediction 180: [ 0.  1.] : 1\n",
      "Prediction 181: [ 0.  1.] : 1\n",
      "Prediction 182: [ 0.  1.] : 1\n",
      "Prediction 183: [ 0.  1.] : 1\n",
      "Prediction 184: [ 0.  1.] : 1\n",
      "Prediction 185: [ 0.  1.] : 1\n",
      "Prediction 186: [ 0.  1.] : 1\n",
      "Prediction 187: [ 0.  1.] : 1\n",
      "Prediction 188: [ 0.  1.] : 1\n",
      "Prediction 189: [ 0.  1.] : 1\n",
      "Prediction 190: [ 0.  1.] : 1\n",
      "Prediction 191: [ 0.  1.] : 1\n",
      "Prediction 192: [ 0.  1.] : 1\n",
      "Prediction 193: [ 0.  1.] : 1\n",
      "Prediction 194: [ 0.  1.] : 1\n",
      "Prediction 195: [ 0.  1.] : 1\n",
      "Prediction 196: [ 0.  1.] : 1\n",
      "Prediction 197: [ 0.  1.] : 1\n",
      "Prediction 198: [ 0.  1.] : 1\n",
      "Prediction 199: [ 0.  1.] : 1\n",
      "Prediction 200: [ 0.  1.] : 1\n",
      "Prediction 201: [ 0.  1.] : 1\n",
      "Prediction 202: [ 0.  1.] : 1\n",
      "Prediction 203: [ 0.  1.] : 1\n",
      "Prediction 204: [ 0.  1.] : 1\n",
      "Prediction 205: [ 0.  1.] : 1\n",
      "Prediction 206: [ 0.  1.] : 1\n",
      "Prediction 207: [ 0.  1.] : 1\n",
      "Prediction 208: [ 0.  1.] : 1\n",
      "Prediction 209: [ 0.  1.] : 1\n",
      "Prediction 210: [ 0.  1.] : 1\n",
      "Prediction 211: [ 0.  1.] : 1\n",
      "Prediction 212: [ 0.  1.] : 1\n",
      "Prediction 213: [ 0.  1.] : 1\n",
      "Prediction 214: [ 0.  1.] : 1\n",
      "Prediction 215: [ 0.  1.] : 1\n",
      "Prediction 216: [ 0.  1.] : 1\n",
      "Prediction 217: [ 0.  1.] : 1\n",
      "Prediction 218: [ 0.  1.] : 1\n",
      "Prediction 219: [ 0.  1.] : 1\n",
      "Prediction 220: [ 0.  1.] : 1\n",
      "Prediction 221: [ 0.  1.] : 1\n",
      "Prediction 222: [ 0.  1.] : 1\n",
      "Prediction 223: [ 0.  1.] : 1\n",
      "Prediction 224: [ 0.  1.] : 1\n",
      "Prediction 225: [ 0.  1.] : 1\n",
      "Prediction 226: [ 0.  1.] : 1\n",
      "Prediction 227: [ 0.  1.] : 1\n",
      "Prediction 228: [ 0.  1.] : 1\n",
      "Prediction 229: [ 0.  1.] : 1\n",
      "Prediction 230: [ 0.  1.] : 1\n",
      "Prediction 231: [ 0.  1.] : 1\n",
      "Prediction 232: [ 0.  1.] : 1\n",
      "Prediction 233: [ 0.  1.] : 1\n",
      "Prediction 234: [ 0.  1.] : 1\n",
      "Prediction 235: [ 0.  1.] : 1\n",
      "Prediction 236: [ 0.  1.] : 1\n",
      "Prediction 237: [ 0.  1.] : 1\n",
      "Prediction 238: [ 0.  1.] : 1\n",
      "Prediction 239: [ 0.  1.] : 1\n",
      "Prediction 240: [ 0.  1.] : 1\n",
      "Prediction 241: [ 0.  1.] : 1\n",
      "Prediction 242: [ 0.  1.] : 1\n",
      "Prediction 243: [ 0.  1.] : 1\n",
      "Prediction 244: [ 0.  1.] : 1\n",
      "Prediction 245: [ 0.  1.] : 1\n",
      "Prediction 246: [ 0.  1.] : 1\n",
      "Prediction 247: [ 0.  1.] : 1\n",
      "Prediction 248: [ 0.  1.] : 1\n",
      "Prediction 249: [ 0.  1.] : 1\n",
      "Prediction 250: [ 0.  1.] : 1\n"
     ]
    }
   ],
   "source": [
    "# Predict the outcome of test data using model\n",
    "predictions = nn.predict(X_test, as_iterable=True)\n",
    "y_pred = []\n",
    "for i, p in enumerate(predictions):\n",
    "    y_pred.append(p['labels'])\n",
    "    print(\"Prediction %s: %s : %s\" % (i + 1, p['probs'], p['labels']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = metrics.accuracy_score(np.argmax(y_test,1), y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a313445e787b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'score' is not defined"
     ]
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "\n",
    "import cv2                 # working with, mainly resizing, images\n",
    "import numpy as np\n",
    "import os\n",
    "from random import shuffle # randomizing ordered data\n",
    "from tqdm import tqdm      # a nice pretty percentage bar for tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define variables \n",
    "\n",
    "IMG_WIDTH, IMG_HEIGHT = 100, 100 # dimensions of our images.\n",
    "source_dir_name = '/home/datascience/projects/vsoft/data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One hot encoding of the target colum\n",
    "def label_img(img, word_label):\n",
    "    if word_label == 'Pepperoni': return [1,0]\n",
    "    elif word_label == 'Sausage': return [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create training data\n",
    "def create_train_data(directory='', label='', num_of_images=1000):\n",
    "    training_data = []\n",
    "    if directory=='':\n",
    "        directory = TRAIN_DIR\n",
    "    label_dir = directory+'/'+label\n",
    "    i=0\n",
    "    for img in tqdm(os.listdir(label_dir)):\n",
    "        img_label = label_img(img, label)\n",
    "        path = os.path.join(label_dir,img)\n",
    "        #img = cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.imread(path,cv2.IMREAD_COLOR)\n",
    "        #print(img.shape)\n",
    "        img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
    "        #print(img.shape)\n",
    "        training_data.append([np.array(img),np.array(img_label)])\n",
    "        i=i+1\n",
    "        if i>num_of_images:\n",
    "            break\n",
    "    #print(np.array(training_data).shape)\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 49.41it/s]\n"
     ]
    }
   ],
   "source": [
    "pepperoni_data_test = create_train_data(source_dir_name,'Pepperoni',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 73.34it/s]\n"
     ]
    }
   ],
   "source": [
    "sausage_data_test = create_train_data(source_dir_name,'Sausage',1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data_unseen = np.concatenate((pepperoni_data_test, sausage_data_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_unseen = np.array([i[0] for i in test_data_unseen], dtype='f').reshape(-1, IMG_WIDTH, IMG_HEIGHT, 3)\n",
    "y_test_unseen = np.array([i[1] for i in test_data_unseen], dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object BaseEstimator._predict_generator at 0x7f2f9771d9e8>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_unseen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predict the outcome of test data using model\n",
    "predictions_unseen = nn.predict(X_test_unseen, as_iterable=True)\n",
    "y_pred_unseen = []\n",
    "for i, p in enumerate(predictions_unseen):\n",
    "    y_pred_unseen.append(p['labels'])\n",
    "    print(\"Prediction %s: %s : %s\" % (i + 1, p['probs'], p['labels']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = metrics.accuracy_score(np.argmax(y_test_unseen,1), y_pred_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a313445e787b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'score' is not defined"
     ]
    }
   ],
   "source": [
    "score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
